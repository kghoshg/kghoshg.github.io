<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>COMP 5704</title>


  

  
  
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"></head><body style="color: rgb(0, 0, 0); background-color: rgb(255, 255, 255);" alink="#000000" link="#000000" vlink="#000000">
<table border="0" width="100%">

  <tbody>
    <tr>
      <td height="27" width="45%">
      <h2>COMP 5704: Parallel Algorithms and Applications in Data Science<br>
</h2>
      </td>
      <td height="27" width="10%">
      <p><br>
      </p>
      </td>
      <td height="27" width="45%">
      <p><b>School of Computer Science</b><br>
      <b>Carleton University, Ottawa, Canada</b></p>
      </td>
    </tr>
  </tbody>
</table>

<hr noshade="noshade">
<h2><font color="#005128">Project Title: </font><font><font color="#005128">Newton-ADMM: A Distributed GPU-Accelerated
Optimizer for Non-Convex Problems</font></font></h2>

<h2><font><font color="#005128">Name: K. Ghosh</font></font></h2>

<h2><font><font color="#005128">E-Mail: akademik DOT gk AT gmail DOT com</font></font></h2>



<hr noshade="noshade">
<b><font color="#005128">Project Outline:</font></b> <br/>
In machine learning applications, the first-order optimization techniques - such as gradient descent (SGD) and its variants - are very much employed. For, their simplicity and it costs less per-iteration. However, in distributed environments, they often need a massive number of iterations incurring associated communication costs. On the other hand, Newton-type methods, which have higher per-iteration computation costs, generally need a much smaller number of iterations; thus it reduces the communication costs. 
To solve the aforementioned issue, Chih-Hao Fang et al - in a recent paper - presented a new distributed optimizer for classification problems, which associates a GPU-accelerated Newton-type solver with the global consensus formulation of Alternating Direction of Method Multipliers (ADMM). 
For this project, I would like to extend their work to non-convex problems generated by deep neural networks by incorporating serial nonconvex solvers into their distributed framework.
<p>
	<b><font color="#005128">Startup Paper(s):</font></b> 
	<ul style="list-style-type:circle;">
	  <li>C. Fang, S. Kylasa, F. Roosta, M. Mahoney and A. Grama,  "Newton-ADMM: A Distributed GPU-Accelerated Optimizer for Multiclass Classification Problems <a href="https://arxiv.org/pdf/1807.07132.pdf"> [PDF]</a></li>
	  <li>Fred Roosta, Yang Liu, Peng Xu, and Michael
W Mahoney. "Newton-MR: Newton's Method Without
Smoothness or Convexity" <a href="https://arxiv.org/pdf/1810.00303.pdf"> [PDF]</a></li>
	  <li>Kylasa, Sudhir B. (2019): HIGHER ORDER OPTIMIZATION TECHNIQUES FOR MACHINE LEARNING. Purdue University Graduate School. Thesis.<a href="https://doi.org/10.25394/PGS.11328545.v1">[PDF]</a></li>
	</ul>  
</p>

<p><b><font color="#005128">Deliverables:</font></b></p>

<ul>

  <li>
    <div align="left"><a href="Literature_Review.pdf"><font color="#000000">Literature Review</font></a> (PDF file created from
LATEX template)</div>
  </li>
  <li>
    <div align="left"><a href="Presentation_Outline.pdf"><font color="#000000">Presentation Outline</font></a> (PDF file created from
LATEX template)</div>
  </li>
  <li>
    <div align="left"> <a href="COMP-5704  Project_Presentation_Kuntal_Ghosh.pdf"><font color="#000000">Slide Presentation</font></a> (PowerPoint File) incl.
Question Sheet </div>
  </li>
  <li>
    <div align="left"><a href="Final_Paper.pdf"><font color="#000000">Final
Paper</font></a> (PDF file created from LATEX template)</div>
  </li>
  <li><a href="Code_and_Data"><font color="#000000">Code and Data</font></a>
(put all code, data, etc. into this directory)</li>
</ul>

<p><b><font color="#005128">Relevant References:</font></b></p>

<ul>

  <li>S´ebastien Bubeck et al. "Convex optimization: Algorithms
and complexity". In: Foundations and Trends®
in Machine Learning 8.3-4 (2015), pp. 231-357.</li>

<li>Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato,
Jonathan Eckstein, et al. "Distributed optimization and
statistical learning via the alternating direction method
of multipliers". In: Foundations and Trends® in Machine
learning 3.1 (2011), pp. 1-122. </li>

<li>Hadi Daneshmand, Aurelien Lucchi, and Thomas
Hofmann. "DynaNewton-Accelerating Newton's
Method for Machine Learning". In: arXiv preprint
arXiv:1605.06561 (2016). </li>

<li>Fred Roosta, Yang Liu, Peng Xu, and Michael
W Mahoney. "Newton-MR: Newton's Method Without
Smoothness or Convexity". In: arXiv preprint
arXiv:1810.00303 (2018). </li>

<li>Sudhir B Kylasa. "HIGHER ORDER OPTIMIZATION
TECHNIQUES FOR MACHINE LEARNING". PhD
thesis. Purdue University Graduate School, 2019. </li>

<li>Peng Xu, Farbod Roosta-Khorasani, and Michael W.
Mahoney. "Second-Order Optimization for Non-Convex
Machine Learning: An Empirical Study". In: arXiv
preprint arXiv:1708.07827 (2017). </li>

<li>Yuchen Zhang and Xiao Lin. "DiSCO: Distributed
optimization for self-concordant empirical loss". In:
International conference on machine learning. 2015,
pp. 362-370. </li>

<li>Celestine D¨unner, Aurelien Lucchi, Matilde Gargiani,
An Bian, Thomas Hofmann, and Martin Jaggi. "A
Distributed Second-Order Algorithm You Can Trust".
In: arXiv preprint arXiv:1806.07569 (2018). </li>

<li>L´eon Bottou, Frank E Curtis, and Jorge Nocedal. "Optimization
methods for large-scale machine learning".
In: arXiv preprint arXiv:1606.04838 (2016). </li>

<li>Hadi Daneshmand, Aurelien Lucchi, and Thomas
Hofmann. "DynaNewton-Accelerating Newton's
Method for Machine Learning". In: arXiv preprint
arXiv:1605.06561 (2016). </li>

<li> Fred Roosta, Yang Liu, Peng Xu, and Michael W Mahoney. "Newton-MR: Newton's Method Without Smoothness or Convexity". In: arXiv preprint arXiv:1810.00303 (2018).? </li>

<li> L. Angelani, R. Di Leonardo, G. Ruocco, A. Scala, and F. Sciortino. Saddles in the Energy Landscape Probed by Supercooled Liquids. Physical review letters, 85(25):5356, 2000. </li>

<li> Y. Arjevani, O. Shamir, and R. Shi. Oracle Complexity of Second-Order Methods for Smooth Convex Optimization. Mathematical Programming, pages 1{34, 2017.? </li>

<li> Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. "Revisiting distributed synchronous SGD". In: arXiv preprint arXiv:1604.00981 (2016).? </li>

<li> Y. Bengio et al. Learning deep architectures for AI. Foundations and trends R in Machine Learning, 2(1):1{127, 2009.? </li>

<li> S. Bellavia, C. Cartis, N. I. M. Gould, B. Morini, and Ph. L. Toint. Convergence of a regularized Euclidean residual algorithm for nonlinear least-squares. SIAM Journal on Numerical Analysis, 48(1):1{29, 2010.? </li>

<li> Groueix, T., Fisher, M., Kim, V., Russell, B., Aubry, M.: Atlasnet: A papier-m^ache approach to learning 3d surface generation. In: CVPR 2018 (2018)? </li>

<li> Rixon Crane and Fred Roosta. "DINGO: Distributed Newton-Type Method for Gradient-Norm Optimization". In: Proceedings of the Advances in Neural Information Processing Systems. Accepted. 2019.? </li>

<li> D. Calvetti, B. Lewis, and L. Reichel. L-curve for the MINRES method. In Advanced Signal Processing Algorithms, Architectures, and Implementations X, volume 4116, pages 385{396. International Society for Optics and Photonics, 2000.? </li>

<li> Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. "Accurate, large minibatch SGD: training imagenet in 1 hour". In: arXiv preprint arXiv:1706.02677 (2017).? </li>

<li> K. v. d. Doel and U. Ascher. Adaptive and stochastic algorithms for EIT and DC resistivity problems with piecewise constant solutions and many measurements. SIAM J. Scient. Comput., 34:DOI: 10.1137/110826692 2012? </li>

<li> Sashank J Reddi, Jakub Kone?cn`y, Peter Richt´arik, Barnab´as P´ocz´os, and Alex Smola. "AIDE: Fast and communication efficient distributed optimization". In: arXiv preprint arXiv:1608.06879 (2016).? </li>

<li> B. Kylasa, F. Roosta-Khorasani, M. W. Mahoney, and A. Grama. GPU Accelerated Sub-Sampled Newton's Method. arXiv preprint arXiv:1802.09113. Accepted for publication in the Proceedings of SIAM SDM 2019.? </li>

<li> H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods under the Polyak- Lojasiewicz condition. Joint European Conference on Machine Learning and Knowledge Discovery in Databases,pages 795{811, 2016.? </li>

</ul>

</body></html>